{"cells":[{"cell_type":"code","metadata":{"tags":[],"source_hash":"bd270f23","execution_start":1671051772840,"execution_millis":5082,"deepnote_to_be_reexecuted":false,"cell_id":"8e3f574d586544ebad3bf78de00805cc","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport tensorflow_model_optimization as tfmot\n\nfrom preprocessing import LABELS\nfrom preprocessing import *\nfrom functools import partial\n\nfrom typing import Iterable, Any\nfrom itertools import product\n\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport random\nimport zipfile\nfrom glob import glob\nimport os\n\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n# tf.autograph.set_verbosity(2)\n","execution_count":1,"outputs":[{"name":"stderr","text":"2022-12-14 21:02:52.808729: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-14 21:02:52.908266: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-12-14 21:02:52.912807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-12-14 21:02:52.912823: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-14 21:02:52.935647: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-12-14 21:02:54.534510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-12-14 21:02:54.534564: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-12-14 21:02:54.534570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"3077601b","execution_start":1671051777966,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"75564b1d25404e02af5670647523a52a","deepnote_cell_type":"code"},"source":"seed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"2993ba3b","execution_start":1671051777967,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"c58f315222d1462498e67d15617bdf37","deepnote_cell_type":"code"},"source":"# !ls /datasets/minispeachcommands\n# !unzip -q /datasets/minispeechcommands/msc-train.zip\n# !unzip -q /datasets/minispeechcommands/msc-val.zip\n# !unzip -q /datasets/minispeechcommands/msc-test.zip","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"80a10e69","execution_start":1671051777967,"execution_millis":365,"deepnote_to_be_reexecuted":false,"cell_id":"3f692420a5774083a6c45b152ee69474","deepnote_cell_type":"code"},"source":"go_stop_train_ds_pure = tf.data.Dataset.list_files(['msc-train/go*', 'msc-train/stop*'])\ngo_stop_val_ds_pure = tf.data.Dataset.list_files(['msc-val/go*', 'msc-val/stop*'])\ngo_stop_test_ds_pure = tf.data.Dataset.list_files(['msc-test/go*', 'msc-test/stop*'])\n\nnum_train_files = len(go_stop_train_ds_pure)\nnum_val_files = len(go_stop_val_ds_pure)\nnum_test_files = len(go_stop_test_ds_pure)\n\nprint('Training set size:', num_train_files)\nprint('Validation set size:', num_val_files)\nprint('Test set size:', num_test_files)\n\ngo_stop_label = [LABELS[1], LABELS[5]]\nprint(go_stop_label)","execution_count":4,"outputs":[{"name":"stderr","text":"2022-12-14 21:02:57.937932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-12-14 21:02:57.937957: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-14 21:02:57.937971: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-5c1936eb-da68-471b-86ff-fc150fc4e316): /proc/driver/nvidia/version does not exist\n2022-12-14 21:02:57.938239: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nTraining set size: 1600\nValidation set size: 200\nTest set size: 200\n['go', 'stop']\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"b0cc0080","execution_start":1671051778333,"execution_millis":1,"deepnote_to_be_reexecuted":false,"cell_id":"108b03ab19254237be074c9d2a795b91","deepnote_cell_type":"code"},"source":"global get_frozen_spectrogram\n\ndef get_mfcc_and_label(filename, downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency, num_coefficients):\n    mfccs, label = get_mfccs(filename, downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency, num_coefficients)\n    \n    return mfccs, label\n\ndef preprocess(filename):\n    signal, label = get_frozen_spectrogram(filename)\n\n    signal.set_shape(SHAPE)\n    signal = tf.expand_dims(signal, -1)\n    signal = tf.image.resize(signal, [32,32])\n    # we put SHAPE instead of [32,32] since for testing part(with loading tflite) input must be [49,40,1]\n    # so input for creating model is  [49,40,1] not [32,32,1]\n\n\n    label_id = tf.argmax(label == go_stop_label)\n\n    return signal, label_id","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"7c79a034","execution_start":1671051778350,"execution_millis":1,"deepnote_to_be_reexecuted":false,"cell_id":"608c03d634e24944ba35846955776bd6","deepnote_cell_type":"code"},"source":"def test_acc_latency(interpreter, input_details, output_details, filenames, PREPROCESSING_ARGS):\n\n    avg_preprocessing_latency = 0\n    avg_model_latency = 0\n    latencies = []\n    accuracy = 0.0\n\n    downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n    sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n    frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n    frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n    num_spectrogram_bins = frame_length // 2 + 1\n    num_mel_bins = PREPROCESSING_ARGS['num_mel_bins']\n    lower_frequency = PREPROCESSING_ARGS['lower_frequency']\n    upper_frequency = PREPROCESSING_ARGS['upper_frequency']\n\n    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n        num_mel_bins=num_mel_bins,\n        num_spectrogram_bins=num_spectrogram_bins,\n        sample_rate=downsampling_rate,\n        lower_edge_hertz=lower_frequency,\n        upper_edge_hertz=upper_frequency\n    )\n\n    print('filename len:', len(filenames))\n    for filename in filenames:\n        \n        audio_binary = tf.io.read_file(filename)\n\n        # NEED ONLY FOR TESTING\n        path_parts = tf.strings.split(filename, '/')\n        path_end = path_parts[-1]\n        file_parts = tf.strings.split(path_end, '_')\n        true_label = file_parts[0]\n        true_label = true_label.numpy().decode()\n\n        # PRE-PROCESSING (LOG-MEL SPECTROGRAM)\n        start_preprocess = time()\n        audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n        audio = tf.squeeze(audio)\n\n        zero_padding = tf.zeros(sampling_rate - tf.shape(audio), dtype=tf.float32)\n        audio_padded = tf.concat([audio, zero_padding], axis=0)\n\n        if downsampling_rate != sampling_rate:\n            audio_padded = tfio.audio.resample(audio_padded, sampling_rate_int64, downsampling_rate)\n\n        stft = tf.signal.stft(\n            audio_padded,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            fft_length=frame_length\n        )\n        spectrogram = tf.abs(stft)\n\n        mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n        log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n\n        mfccs = tf.expand_dims(mfccs, 0)  # batch axis\n        mfccs = tf.expand_dims(mfccs, -1)  # channel axis\n        mfccs = tf.image.resize(mfccs, [32,32])\n\n        end_preprocess = time()\n\n        interpreter.set_tensor(input_details[0]['index'], mfccs)\n        interpreter.invoke()\n        output = interpreter.get_tensor(output_details[0]['index'])\n\n        end_inference = time()\n\n        top_index = np.argmax(output[0])\n        predicted_label = go_stop_label[top_index]\n\n        accuracy += true_label == predicted_label\n        avg_preprocessing_latency += (end_preprocess - start_preprocess)\n        avg_model_latency += (end_inference - end_preprocess)\n        latencies.append(end_inference - start_preprocess)\n\n    accuracy /= len(filenames)\n    avg_preprocessing_latency /= len(filenames)\n    avg_model_latency /= len(filenames)\n    median_total_latency = np.median(latencies)\n\n    return accuracy, avg_preprocessing_latency, avg_model_latency, median_total_latency\n","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"d68fef32","execution_start":1671051778351,"execution_millis":21,"deepnote_to_be_reexecuted":false,"cell_id":"f885543053184e1dbfa42384632eb247","deepnote_cell_type":"code"},"source":"def simulator(downsampling_rate,frame_length_in_s, num_mel_bins, lower_frequency, upper_frequency,\n                batch_size, initial_learning_rate, end_learning_rate, epochs, model_filter_param, alpha_param, final_sparsity_param):\n\n    global get_frozen_spectrogram, SHAPE, alpha, final_sparsity, model_filter\n\n    model_filter = model_filter_param\n    alpha = alpha_param\n    final_sparsity = final_sparsity_param\n    frame_step_in_s_list = [frame_length_in_s]\n    num_coefficients = num_mel_bins\n\n    # **************************Model introducing\n\n    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n    metrics = [tf.metrics.SparseCategoricalAccuracy()]\n    callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n\n    model_pure = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=[32,32, 1]),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(units=len(go_stop_label)),\n        tf.keras.layers.Softmax()\n    ])\n\n\n\n    model_DS_CNN = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=[32,32, 1]),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[1, 1], strides=[1, 1], use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[1, 1], strides=[1, 1], use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(units=len(go_stop_label)),\n        tf.keras.layers.Softmax()\n    ])\n\n    model_Width_Scaling = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=[32,32, 1]),\n        tf.keras.layers.Conv2D(filters=int(model_filter * alpha), kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=int(model_filter * alpha), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=int(model_filter * alpha), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(units=len(go_stop_label)),\n        tf.keras.layers.Softmax()\n    ])\n\n\n    model_Width_Pruning = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=[32,32, 1]),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=model_filter, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(units=len(go_stop_label)),\n        tf.keras.layers.Softmax()\n    ])\n\n\n\n    for frame_step_in_s in frame_step_in_s_list:\n        \n        # **************************Copy the original data as local variable\n        go_stop_train_ds = go_stop_train_ds_pure\n        go_stop_val_ds = go_stop_val_ds_pure\n        go_stop_test_ds = go_stop_test_ds_pure\n        go_stop_test_ds_tflit = go_stop_test_ds_pure\n\n        # **************************Definition of parameters\n        PREPROCESSING_ARGS = {\n            'downsampling_rate': downsampling_rate,\n            'frame_length_in_s': frame_length_in_s,\n            'frame_step_in_s': frame_step_in_s,\n            'num_mel_bins': num_mel_bins,\n            'num_coefficients': num_coefficients,\n            'lower_frequency': lower_frequency,\n            'upper_frequency': upper_frequency,\n            \n        }\n\n        TRAINING_ARGS = {\n            'batch_size': batch_size,\n            'initial_learning_rate': initial_learning_rate,\n            'end_learning_rate': end_learning_rate,\n            'epochs': epochs\n        }\n        \n        # **************************Getting mcff \n        global get_frozen_spectrogram, SHAPE\n\n        get_frozen_spectrogram = partial(get_mfcc_and_label, **PREPROCESSING_ARGS)\n\n        for mfcc, label in go_stop_train_ds.map(get_frozen_spectrogram).take(1):\n            mfcc_sample = mfcc\n            SHAPE = mfcc[:num_coefficients].shape\n            print('SHAPE = ', SHAPE)\n            \n        \n        \n        # **************************Rezising and batching\n        batch_size = batch_size\n        epochs = epochs\n\n        go_stop_train_ds = go_stop_train_ds.map(preprocess).batch(batch_size).cache()\n        go_stop_val_ds = go_stop_val_ds.map(preprocess).batch(batch_size)\n        go_stop_test_ds = go_stop_test_ds.map(preprocess).batch(batch_size)\n\n        for example_batch, example_labels in go_stop_train_ds.take(1):\n            print('Batch Shape:', example_batch.shape)\n            print('Data Shape:', example_batch.shape[1:])\n            print('Labels:', example_labels)\n        \n\n        # **************************Model definition\n        # Preparation of optimizer\n        linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n            initial_learning_rate=initial_learning_rate,\n            end_learning_rate=end_learning_rate,\n            decay_steps=len(go_stop_train_ds) * epochs,\n        )\n        optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n\n\n        prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\n        begin_step = int(len(go_stop_train_ds_pure) * epochs * 0.2)\n        end_step = int(len(go_stop_train_ds_pure) * epochs)\n\n        # Preparation of pruning model\n        pruning_params = {\n            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n                initial_sparsity=0.20,\n                final_sparsity=final_sparsity,\n                begin_step=begin_step,\n                end_step=end_step\n            )\n        }\n\n\n        # **************************Pick a model to compile\n        model_list = [model_pure, model_DS_CNN, model_Width_Scaling]\n        # model = model_list[2]\n        model_for_pruning = prune_low_magnitude(model_Width_Scaling, **pruning_params)  # changeeeeeeeeeeeeeeeeeeeeeee\n\n        # model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n        model_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics) # changeeeeeeeeeeeeeeeeeeeeeee\n\n        # **************************Start traning\n        # history = model.fit(go_stop_train_ds, epochs=epochs, validation_data=go_stop_val_ds)\n        history = model_for_pruning.fit(go_stop_train_ds, epochs=epochs, validation_data=go_stop_val_ds, callbacks=callbacks) # changeeeeeeeeeeeeeeeeeeeeeee\n        \n        # **************************Results of training\n        training_loss = history.history['loss'][-1]\n        training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n        val_loss = history.history['val_loss'][-1]\n        val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n        \n        # print(f'Model name is: {model}')\n        print(f'Training Loss: {training_loss:.4f}')\n        print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n        print()\n        print(f'Validation Loss: {val_loss:.4f}')\n        print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n        print()\n        \n        # **************************Model evaluation without saving and latency\n        # test_loss, test_accuracy = model.evaluate(go_stop_test_ds)\n        test_loss, test_accuracy = model_for_pruning.evaluate(go_stop_test_ds) # changeeeeeeeeeeeeeeeeeeeeeee\n\n        print(f'Test Loss: {test_loss:.4f}')\n        print(f'Test Accuracy: {test_accuracy*100.:.2f}%')\n        \n        \n        # **************************Model saving as tflite\n        timestamp = int(time())\n\n        saved_model_dir = f'./team10_final_8000/saved_models_grid_search/{timestamp}'\n        if not os.path.exists(saved_model_dir):\n            os.makedirs(saved_model_dir)\n        \n        # model.save(saved_model_dir)\n        model_for_pruning.save(saved_model_dir) # changeeeeeeeeeeeeeeeeeeeeeee\n\n        converter = tf.lite.TFLiteConverter.from_saved_model(f'./team10_final_8000/saved_models_grid_search/{timestamp}')\n        tflite_model = converter.convert()\n\n        tflite_models_dir = './team10_final_8000/saved_models_grid_search_tflite_models'\n        if not os.path.exists(tflite_models_dir):\n            os.makedirs(tflite_models_dir)\n        \n        tflite_model_name = os.path.join(tflite_models_dir, f'{timestamp}.tflite')\n        tflite_model_name\n\n        with open(tflite_model_name, 'wb') as fp:\n            fp.write(tflite_model)\n\n        with zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n            f.write(tflite_model_name)\n\n        zipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0\n\n\n        # **************************Model evaluation with loading tflite and latency\n        interpreter = tf.lite.Interpreter(model_path=f'./team10_final_8000/saved_models_grid_search_tflite_models/{timestamp}.tflite')\n        interpreter.allocate_tensors()\n\n        input_details = interpreter.get_input_details()\n        output_details = interpreter.get_output_details()\n\n        print(\"Number of inputs:\", len(input_details))\n        print(\"Number of outputs:\", len(output_details))\n        print(\"Input name:\", input_details[0]['name'])\n        print(\"Input shape:\", input_details[0]['shape'])\n        print(\"Output name:\", output_details[0]['name'])\n        print(\"Output shape:\", output_details[0]['shape'])\n\n        accuracy, avg_preprocessing_latency, avg_model_latency, median_total_latency = test_acc_latency(interpreter, input_details, output_details, go_stop_test_ds_tflit, PREPROCESSING_ARGS)\n        model_size = os.path.getsize(f'./team10_final_8000/saved_models_grid_search_tflite_models/{timestamp}.tflite')\n        \n\n        print(f'Accuracy: {100 * accuracy:.3f}%')\n        print(f'Model size: {model_size / 2 ** 10:.1f}KB')\n        print(f'Zipped tflite size: {zipped_size:.3f} KB')\n        print(f'Preprocessing Latency: {1000 * avg_preprocessing_latency:.1f}ms')\n        print(f'Model Latency: {1000 * avg_model_latency:.1f}ms')\n        print(f'Total Latency: {1000 * median_total_latency:.1f}ms')\n\n        output_dict = {\n            'timestamp_model_name': timestamp,\n            **PREPROCESSING_ARGS,\n            **TRAINING_ARGS,\n            'test_accuracy': test_accuracy,\n            'accuracy': accuracy,\n            'avg_preprocessing_latency': avg_preprocessing_latency,\n            'avg_model_latency': avg_model_latency,\n            'median_total_latency': median_total_latency,\n            'Model size': model_size / 2 ** 10,\n            'Zipped tflite size': zipped_size,\n            'model_filter': model_filter,\n            'alpha': alpha,\n            'final_sparsity': final_sparsity\n        }\n\n        df = pd.DataFrame([output_dict])\n\n        saved_out_put_dir = './team10_final_8000/out_put_dict_grid_search'\n        if not os.path.exists(saved_out_put_dir):\n            os.makedirs(saved_out_put_dir)\n\n        # output_path = f'./team10_testing/out_put_dict_grid_search/spectrogram_results_width_scaling.csv'\n        output_path = f'./team10_final_8000/out_put_dict_grid_search/spectrogram_results_pruning_width_scaling.csv'\n        df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)\n\n","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"f940fec7","execution_start":1671051778420,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"49e56c41df0142388c0ddb4b6e9a86c0","deepnote_cell_type":"code"},"source":"    # 'downsampling_rate': [16000], # [8000, 16000]\n    # 'frame_length_in_s': [0.016], # [0.08, 0.016, 0.04, 0.08]\n    # # 'frame_step_in_s': [0.02], # using [0.25, 0.5 ,1] as coefficient of frame_length_in_s\n    # 'num_mel_bins': [20], # [20, 40]\n    # 'lower_frequency': [20],\n    # 'upper_frequency': [8000], # [4000, 8000]\n    # # 'num_coefficients': [20], # using [0.5 ,1] as coefficient of num_mel_bins\n    # 'batch_size': [20], # [10, 20, 30]\n    # 'initial_learning_rate': [0.01],\n    # 'end_learning_rate': [1.e-5],\n    # 'epochs': [20], # [10, 20, 30]\n    # 'model_filter':[64, 128, 256],\n    # 'alpha': [0.05, 0.1, 0.2, 0.3],\n    # # 'alpha_DS': [0.05],\n    # 'final_sparsity': [0.5, 0.6, 0.7, 0.8],\n    # # 'final_sparsity_DS': [0.6],\n\nPREPROCESSING_TRAINING_ARGS = {\n    'downsampling_rate': [16000],\n    'frame_length_in_s': [0.016],\n    # 'frame_step_in_s': [0.02],\n    'num_mel_bins': [20],\n    'lower_frequency': [20],\n    'upper_frequency': [8000],\n    # 'num_coefficients': [20],\n    'batch_size': [20],\n    'initial_learning_rate': [0.01],\n    'end_learning_rate': [1.e-5],\n    'epochs': [20],\n    'model_filter':[64, 128, 256],\n    'alpha': [0.05, 0.1, 0.2, 0.3],\n    # 'alpha_DS': [0.05],\n    'final_sparsity': [0.5, 0.6, 0.7, 0.8],\n    # 'final_sparsity_DS': [0.6],\n\n}\n\ndef grid_parameters(parameters: dict[str, Iterable[Any]]) -> Iterable[dict[str, Any]]:\n    for params in product(*parameters.values()):\n        yield dict(zip(parameters.keys(), params))","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"c316843f","execution_start":1671051778420,"execution_millis":3811137,"deepnote_to_be_reexecuted":false,"cell_id":"aade7a369d884b79aae81496e13504c0","deepnote_cell_type":"code"},"source":"for parameters in grid_parameters(PREPROCESSING_TRAINING_ARGS):\n    print('parameters are: ', parameters)\n    simulator(*parameters.values())\n    # print('parameters are: ', parameters)\n    # break","execution_count":9,"outputs":[{"name":"stdout","text":"Output name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 100.000%\nModel size: 54.6KB\nZipped tflite size: 45.621 KB\nPreprocessing Latency: 5.1ms\nModel Latency: 0.2ms\nTotal Latency: 5.1ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.1, 'final_sparsity': 0.7}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:46:06.916622: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:06.918090: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:06.918251: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:46:07.490909: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:07.492448: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:07.492639: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:46:07.837682: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:07.839139: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:07.839330: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:46:08.750471: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:08.751954: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:46:08.752173: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0], shape=(20,), dtype=int64)\n2022-12-14 21:46:09.328460: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 9s 85ms/step - loss: 0.4615 - sparse_categorical_accuracy: 0.7856 - val_loss: 0.5097 - val_sparse_categorical_accuracy: 0.8100\nEpoch 2/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.2150 - sparse_categorical_accuracy: 0.9312 - val_loss: 1.0871 - val_sparse_categorical_accuracy: 0.6150\nEpoch 3/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.1810 - sparse_categorical_accuracy: 0.9356 - val_loss: 0.2568 - val_sparse_categorical_accuracy: 0.9050\nEpoch 4/20\n80/80 [==============================] - 3s 39ms/step - loss: 0.1332 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.8314 - val_sparse_categorical_accuracy: 0.6700\nEpoch 5/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0989 - sparse_categorical_accuracy: 0.9663 - val_loss: 0.1337 - val_sparse_categorical_accuracy: 0.9550\nEpoch 6/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0820 - sparse_categorical_accuracy: 0.9744 - val_loss: 0.2246 - val_sparse_categorical_accuracy: 0.9200\nEpoch 7/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0750 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.2974 - val_sparse_categorical_accuracy: 0.8850\nEpoch 8/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0734 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.1355 - val_sparse_categorical_accuracy: 0.9500\nEpoch 9/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0608 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.1483 - val_sparse_categorical_accuracy: 0.9450\nEpoch 10/20\n80/80 [==============================] - 3s 39ms/step - loss: 0.0579 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.1270 - val_sparse_categorical_accuracy: 0.9550\nEpoch 11/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0493 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.1457 - val_sparse_categorical_accuracy: 0.9400\nEpoch 12/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0419 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.1475 - val_sparse_categorical_accuracy: 0.9550\nEpoch 13/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0367 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.1233 - val_sparse_categorical_accuracy: 0.9500\nEpoch 14/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.1241 - val_sparse_categorical_accuracy: 0.9500\nEpoch 15/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.1214 - val_sparse_categorical_accuracy: 0.9550\nEpoch 16/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.1197 - val_sparse_categorical_accuracy: 0.9600\nEpoch 17/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.1185 - val_sparse_categorical_accuracy: 0.9600\nEpoch 18/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.1207 - val_sparse_categorical_accuracy: 0.9650\nEpoch 19/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1221 - val_sparse_categorical_accuracy: 0.9650\nEpoch 20/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.1249 - val_sparse_categorical_accuracy: 0.9600\nTraining Loss: 0.0182\nTraining Accuracy: 99.12%\n\nValidation Loss: 0.1249\nValidation Accuracy: 96.00%\n\n10/10 [==============================] - 1s 45ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9950\nTest Loss: 0.0198\nTest Accuracy: 99.50%\nWARNING:absl:Found untraced functions such as conv2d_462_layer_call_fn, conv2d_462_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_462_layer_call_fn, re_lu_462_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054441/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054441/assets\n2022-12-14 21:47:26.795584: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 21:47:26.795620: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 21:47:26.795753: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671054441\n2022-12-14 21:47:26.804038: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 21:47:26.804066: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671054441\n2022-12-14 21:47:26.829223: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 21:47:26.912869: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671054441\n2022-12-14 21:47:26.944629: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 148876 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_155:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 99.500%\nModel size: 54.6KB\nZipped tflite size: 45.612 KB\nPreprocessing Latency: 5.1ms\nModel Latency: 0.2ms\nTotal Latency: 5.2ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.1, 'final_sparsity': 0.8}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:47:29.732052: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:29.733622: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:29.733857: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:47:30.334670: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:30.336150: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:30.336327: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:47:30.673470: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:30.674912: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:30.675092: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:47:31.001218: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:31.002613: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:47:31.002796: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0], shape=(20,), dtype=int64)\n2022-12-14 21:47:31.431502: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 9s 86ms/step - loss: 0.4170 - sparse_categorical_accuracy: 0.8231 - val_loss: 1.7179 - val_sparse_categorical_accuracy: 0.5200\nEpoch 2/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.2050 - sparse_categorical_accuracy: 0.9219 - val_loss: 1.4791 - val_sparse_categorical_accuracy: 0.5650\nEpoch 3/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.1489 - sparse_categorical_accuracy: 0.9456 - val_loss: 0.2379 - val_sparse_categorical_accuracy: 0.9000\nEpoch 4/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.1087 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.1392 - val_sparse_categorical_accuracy: 0.9450\nEpoch 5/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.0894 - sparse_categorical_accuracy: 0.9644 - val_loss: 0.2117 - val_sparse_categorical_accuracy: 0.9150\nEpoch 6/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.0755 - sparse_categorical_accuracy: 0.9694 - val_loss: 0.2183 - val_sparse_categorical_accuracy: 0.9350\nEpoch 7/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.0613 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.1646 - val_sparse_categorical_accuracy: 0.9400\nEpoch 8/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.0523 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.2365 - val_sparse_categorical_accuracy: 0.9300\nEpoch 9/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0442 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.1296 - val_sparse_categorical_accuracy: 0.9600\nEpoch 10/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.1767 - val_sparse_categorical_accuracy: 0.9550\nEpoch 11/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.0366 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.1511 - val_sparse_categorical_accuracy: 0.9600\nEpoch 12/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0310 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.2122 - val_sparse_categorical_accuracy: 0.9400\nEpoch 13/20\n80/80 [==============================] - 3s 40ms/step - loss: 0.0264 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.1446 - val_sparse_categorical_accuracy: 0.9600\nEpoch 14/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1655 - val_sparse_categorical_accuracy: 0.9600\nEpoch 15/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.1450 - val_sparse_categorical_accuracy: 0.9600\nEpoch 16/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.1491 - val_sparse_categorical_accuracy: 0.9550\nEpoch 17/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.1515 - val_sparse_categorical_accuracy: 0.9600\nEpoch 18/20\n80/80 [==============================] - 3s 42ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1536 - val_sparse_categorical_accuracy: 0.9600\nEpoch 19/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.1582 - val_sparse_categorical_accuracy: 0.9550\nEpoch 20/20\n80/80 [==============================] - 3s 41ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1624 - val_sparse_categorical_accuracy: 0.9550\nTraining Loss: 0.0138\nTraining Accuracy: 99.50%\n\nValidation Loss: 0.1624\nValidation Accuracy: 95.50%\n\n10/10 [==============================] - 1s 44ms/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9900\nTest Loss: 0.0269\nTest Accuracy: 99.00%\nWARNING:absl:Found untraced functions such as conv2d_474_layer_call_fn, conv2d_474_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_474_layer_call_fn, re_lu_474_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054524/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054524/assets\n2022-12-14 21:48:50.010334: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 21:48:50.010372: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 21:48:50.010503: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671054524\n2022-12-14 21:48:50.018507: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 21:48:50.018535: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671054524\n2022-12-14 21:48:50.043004: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 21:48:50.129545: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671054524\n2022-12-14 21:48:50.158629: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 148126 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_159:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 99.000%\nModel size: 54.6KB\nZipped tflite size: 45.571 KB\nPreprocessing Latency: 5.1ms\nModel Latency: 0.2ms\nTotal Latency: 5.1ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.2, 'final_sparsity': 0.5}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:48:52.893113: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:52.894612: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:52.894792: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:48:53.503275: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:53.504769: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:53.504993: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:48:53.839217: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:53.840664: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:53.840841: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:48:54.747365: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:54.748886: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:48:54.749075: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1], shape=(20,), dtype=int64)\n2022-12-14 21:48:55.330960: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 11s 109ms/step - loss: 0.4412 - sparse_categorical_accuracy: 0.8025 - val_loss: 1.4206 - val_sparse_categorical_accuracy: 0.7200\nEpoch 2/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.2385 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.2835 - val_sparse_categorical_accuracy: 0.8700\nEpoch 3/20\n80/80 [==============================] - 5s 66ms/step - loss: 0.1488 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.1689 - val_sparse_categorical_accuracy: 0.9400\nEpoch 4/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.1189 - sparse_categorical_accuracy: 0.9575 - val_loss: 0.1389 - val_sparse_categorical_accuracy: 0.9450\nEpoch 5/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0906 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.2001 - val_sparse_categorical_accuracy: 0.9500\nEpoch 6/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0818 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1511 - val_sparse_categorical_accuracy: 0.9450\nEpoch 7/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.1376 - val_sparse_categorical_accuracy: 0.9550\nEpoch 8/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.1512 - val_sparse_categorical_accuracy: 0.9500\nEpoch 9/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0555 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.1300 - val_sparse_categorical_accuracy: 0.9700\nEpoch 10/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.1392 - val_sparse_categorical_accuracy: 0.9650\nEpoch 11/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.7189 - val_sparse_categorical_accuracy: 0.7750\nEpoch 12/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.1330 - val_sparse_categorical_accuracy: 0.9500\nEpoch 13/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1421 - val_sparse_categorical_accuracy: 0.9550\nEpoch 14/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.1325 - val_sparse_categorical_accuracy: 0.9650\nEpoch 15/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1544 - val_sparse_categorical_accuracy: 0.9550\nEpoch 16/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1451 - val_sparse_categorical_accuracy: 0.9600\nEpoch 17/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1454 - val_sparse_categorical_accuracy: 0.9600\nEpoch 18/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1462 - val_sparse_categorical_accuracy: 0.9600\nEpoch 19/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1497 - val_sparse_categorical_accuracy: 0.9600\nEpoch 20/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.1524 - val_sparse_categorical_accuracy: 0.9600\nTraining Loss: 0.0106\nTraining Accuracy: 99.75%\n\nValidation Loss: 0.1524\nValidation Accuracy: 96.00%\n\n10/10 [==============================] - 1s 45ms/step - loss: 0.0167 - sparse_categorical_accuracy: 0.9950\nTest Loss: 0.0167\nTest Accuracy: 99.50%\nWARNING:absl:Found untraced functions such as conv2d_486_layer_call_fn, conv2d_486_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_486_layer_call_fn, re_lu_486_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054643/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054643/assets\n2022-12-14 21:50:48.923960: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 21:50:48.923999: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 21:50:48.924134: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671054643\n2022-12-14 21:50:48.932477: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 21:50:48.932506: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671054643\n2022-12-14 21:50:48.956455: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 21:50:49.040831: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671054643\n2022-12-14 21:50:49.069708: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 145575 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_163:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 99.500%\nModel size: 195.4KB\nZipped tflite size: 176.342 KB\nPreprocessing Latency: 5.2ms\nModel Latency: 0.6ms\nTotal Latency: 5.6ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.2, 'final_sparsity': 0.6}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:50:51.940199: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:51.941813: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:51.942040: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:50:52.578096: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:52.579678: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:52.579869: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:50:52.927474: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:52.929315: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:52.929509: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:50:53.269841: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:53.271417: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:50:53.271620: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0], shape=(20,), dtype=int64)\n2022-12-14 21:50:53.730681: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 12s 112ms/step - loss: 0.4568 - sparse_categorical_accuracy: 0.7969 - val_loss: 0.3620 - val_sparse_categorical_accuracy: 0.8750\nEpoch 2/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.2379 - sparse_categorical_accuracy: 0.9169 - val_loss: 1.0653 - val_sparse_categorical_accuracy: 0.6800\nEpoch 3/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.1530 - sparse_categorical_accuracy: 0.9513 - val_loss: 0.1302 - val_sparse_categorical_accuracy: 0.9350\nEpoch 4/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9594 - val_loss: 0.1637 - val_sparse_categorical_accuracy: 0.9350\nEpoch 5/20\n80/80 [==============================] - 5s 66ms/step - loss: 0.0949 - sparse_categorical_accuracy: 0.9656 - val_loss: 0.1528 - val_sparse_categorical_accuracy: 0.9300\nEpoch 6/20\n80/80 [==============================] - 5s 65ms/step - loss: 0.0754 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.1353 - val_sparse_categorical_accuracy: 0.9450\nEpoch 7/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0717 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.1417 - val_sparse_categorical_accuracy: 0.9350\nEpoch 8/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0527 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.1392 - val_sparse_categorical_accuracy: 0.9450\nEpoch 9/20\n80/80 [==============================] - 5s 66ms/step - loss: 0.0478 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.2013 - val_sparse_categorical_accuracy: 0.9400\nEpoch 10/20\n80/80 [==============================] - 5s 64ms/step - loss: 0.0466 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.2340 - val_sparse_categorical_accuracy: 0.9350\nEpoch 11/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.2517 - val_sparse_categorical_accuracy: 0.9350\nEpoch 12/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0346 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.1677 - val_sparse_categorical_accuracy: 0.9500\nEpoch 13/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.2147 - val_sparse_categorical_accuracy: 0.9450\nEpoch 14/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.1638 - val_sparse_categorical_accuracy: 0.9500\nEpoch 15/20\n80/80 [==============================] - 5s 64ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1714 - val_sparse_categorical_accuracy: 0.9500\nEpoch 16/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.1768 - val_sparse_categorical_accuracy: 0.9550\nEpoch 17/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0134 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.1705 - val_sparse_categorical_accuracy: 0.9550\nEpoch 18/20\n80/80 [==============================] - 5s 64ms/step - loss: 0.0119 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1671 - val_sparse_categorical_accuracy: 0.9450\nEpoch 19/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.1652 - val_sparse_categorical_accuracy: 0.9450\nEpoch 20/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.1669 - val_sparse_categorical_accuracy: 0.9450\nTraining Loss: 0.0097\nTraining Accuracy: 99.94%\n\nValidation Loss: 0.1669\nValidation Accuracy: 94.50%\n\n10/10 [==============================] - 1s 45ms/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9850\nTest Loss: 0.0368\nTest Accuracy: 98.50%\nWARNING:absl:Found untraced functions such as conv2d_498_layer_call_fn, conv2d_498_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_498_layer_call_fn, re_lu_498_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054763/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054763/assets\n2022-12-14 21:52:48.658156: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 21:52:48.658192: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 21:52:48.658326: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671054763\n2022-12-14 21:52:48.666352: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 21:52:48.666380: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671054763\n2022-12-14 21:52:48.691352: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 21:52:48.781659: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671054763\n2022-12-14 21:52:48.811343: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 153018 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_167:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 98.500%\nModel size: 195.4KB\nZipped tflite size: 176.394 KB\nPreprocessing Latency: 5.0ms\nModel Latency: 0.6ms\nTotal Latency: 5.5ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.2, 'final_sparsity': 0.7}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:52:51.630194: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:51.631666: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:51.631831: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:52:52.208151: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:52.209643: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:52.209832: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:52:52.553025: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:52.554585: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:52.554758: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:52:52.888772: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:52.890247: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:52:52.890428: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1], shape=(20,), dtype=int64)\n2022-12-14 21:52:53.330575: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 11s 109ms/step - loss: 0.4180 - sparse_categorical_accuracy: 0.8081 - val_loss: 0.3282 - val_sparse_categorical_accuracy: 0.8800\nEpoch 2/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.2129 - sparse_categorical_accuracy: 0.9231 - val_loss: 0.1273 - val_sparse_categorical_accuracy: 0.9500\nEpoch 3/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.1307 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.1364 - val_sparse_categorical_accuracy: 0.9600\nEpoch 4/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0881 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.1220 - val_sparse_categorical_accuracy: 0.9600\nEpoch 5/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0763 - sparse_categorical_accuracy: 0.9681 - val_loss: 0.1954 - val_sparse_categorical_accuracy: 0.9400\nEpoch 6/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.1397 - val_sparse_categorical_accuracy: 0.9550\nEpoch 7/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0491 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.8300\nEpoch 8/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0522 - sparse_categorical_accuracy: 0.9769 - val_loss: 0.1311 - val_sparse_categorical_accuracy: 0.9650\nEpoch 9/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.1563 - val_sparse_categorical_accuracy: 0.9600\nEpoch 10/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0324 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.2830 - val_sparse_categorical_accuracy: 0.9100\nEpoch 11/20\n80/80 [==============================] - 5s 64ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.1750 - val_sparse_categorical_accuracy: 0.9650\nEpoch 12/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0220 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.1592 - val_sparse_categorical_accuracy: 0.9650\nEpoch 13/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1965 - val_sparse_categorical_accuracy: 0.9600\nEpoch 14/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.2115 - val_sparse_categorical_accuracy: 0.9600\nEpoch 15/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.2007 - val_sparse_categorical_accuracy: 0.9650\nEpoch 16/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.1923 - val_sparse_categorical_accuracy: 0.9650\nEpoch 17/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.1837 - val_sparse_categorical_accuracy: 0.9650\nEpoch 18/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1857 - val_sparse_categorical_accuracy: 0.9700\nEpoch 19/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0094 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1862 - val_sparse_categorical_accuracy: 0.9600\nEpoch 20/20\n80/80 [==============================] - 5s 63ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9975 - val_loss: 0.1877 - val_sparse_categorical_accuracy: 0.9600\nTraining Loss: 0.0082\nTraining Accuracy: 99.75%\n\nValidation Loss: 0.1877\nValidation Accuracy: 96.00%\n\n10/10 [==============================] - 1s 45ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9950\nTest Loss: 0.0125\nTest Accuracy: 99.50%\nWARNING:absl:Found untraced functions such as conv2d_510_layer_call_fn, conv2d_510_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_510_layer_call_fn, re_lu_510_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054882/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054882/assets\n2022-12-14 21:54:47.442834: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 21:54:47.442871: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 21:54:47.443006: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671054882\n2022-12-14 21:54:47.452912: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 21:54:47.452941: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671054882\n2022-12-14 21:54:47.480445: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 21:54:47.573568: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671054882\n2022-12-14 21:54:47.607126: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 164121 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_171:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 99.500%\nModel size: 195.4KB\nZipped tflite size: 176.375 KB\nPreprocessing Latency: 5.1ms\nModel Latency: 0.6ms\nTotal Latency: 5.6ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.2, 'final_sparsity': 0.8}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:54:50.481448: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:50.482913: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:50.483086: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:54:51.093253: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:51.094715: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:51.094879: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:54:51.442043: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:51.443449: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:51.443619: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:54:51.790282: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:51.791771: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:54:51.791960: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0], shape=(20,), dtype=int64)\n2022-12-14 21:54:52.234300: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 11s 107ms/step - loss: 0.3980 - sparse_categorical_accuracy: 0.8288 - val_loss: 0.8145 - val_sparse_categorical_accuracy: 0.7700\nEpoch 2/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.1869 - sparse_categorical_accuracy: 0.9319 - val_loss: 0.1581 - val_sparse_categorical_accuracy: 0.9400\nEpoch 3/20\n80/80 [==============================] - 5s 61ms/step - loss: 0.1385 - sparse_categorical_accuracy: 0.9475 - val_loss: 0.2580 - val_sparse_categorical_accuracy: 0.9000\nEpoch 4/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0998 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.4373 - val_sparse_categorical_accuracy: 0.8400\nEpoch 5/20\n80/80 [==============================] - 5s 61ms/step - loss: 0.0884 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.3046 - val_sparse_categorical_accuracy: 0.8600\nEpoch 6/20\n80/80 [==============================] - 5s 61ms/step - loss: 0.0700 - sparse_categorical_accuracy: 0.9731 - val_loss: 0.6477 - val_sparse_categorical_accuracy: 0.7500\nEpoch 7/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0629 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.5692 - val_sparse_categorical_accuracy: 0.7850\nEpoch 8/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0490 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.1223 - val_sparse_categorical_accuracy: 0.9500\nEpoch 9/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0446 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.1353 - val_sparse_categorical_accuracy: 0.9600\nEpoch 10/20\n80/80 [==============================] - 5s 61ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.1369 - val_sparse_categorical_accuracy: 0.9550\nEpoch 11/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0373 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.1393 - val_sparse_categorical_accuracy: 0.9600\nEpoch 12/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0289 - sparse_categorical_accuracy: 0.9894 - val_loss: 0.1427 - val_sparse_categorical_accuracy: 0.9650\nEpoch 13/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1800 - val_sparse_categorical_accuracy: 0.9500\nEpoch 14/20\n80/80 [==============================] - 5s 61ms/step - loss: 0.0205 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.1673 - val_sparse_categorical_accuracy: 0.9600\nEpoch 15/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.1677 - val_sparse_categorical_accuracy: 0.9550\nEpoch 16/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1698 - val_sparse_categorical_accuracy: 0.9550\nEpoch 17/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1755 - val_sparse_categorical_accuracy: 0.9550\nEpoch 18/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0136 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1803 - val_sparse_categorical_accuracy: 0.9550\nEpoch 19/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1782 - val_sparse_categorical_accuracy: 0.9550\nEpoch 20/20\n80/80 [==============================] - 5s 62ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1772 - val_sparse_categorical_accuracy: 0.9600\nTraining Loss: 0.0118\nTraining Accuracy: 99.62%\n\nValidation Loss: 0.1772\nValidation Accuracy: 96.00%\n\n10/10 [==============================] - 1s 45ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9900\nTest Loss: 0.0218\nTest Accuracy: 99.00%\nWARNING:absl:Found untraced functions such as conv2d_522_layer_call_fn, conv2d_522_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_522_layer_call_fn, re_lu_522_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054997/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671054997/assets\n2022-12-14 21:56:43.813394: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 21:56:43.813429: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 21:56:43.813562: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671054997\n2022-12-14 21:56:43.824536: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 21:56:43.824563: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671054997\n2022-12-14 21:56:43.848358: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 21:56:43.933218: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671054997\n2022-12-14 21:56:43.962131: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 148569 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_175:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 99.000%\nModel size: 195.4KB\nZipped tflite size: 176.488 KB\nPreprocessing Latency: 5.2ms\nModel Latency: 0.6ms\nTotal Latency: 5.7ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.3, 'final_sparsity': 0.5}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:56:46.826105: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:46.828068: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:46.828243: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:56:47.434493: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:47.435947: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:47.436111: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:56:47.781066: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:47.782528: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:47.782716: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:56:48.112142: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:48.113544: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:56:48.113724: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1], shape=(20,), dtype=int64)\n2022-12-14 21:56:48.531576: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 12s 124ms/step - loss: 0.4648 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.5220 - val_sparse_categorical_accuracy: 0.8200\nEpoch 2/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.2713 - sparse_categorical_accuracy: 0.8944 - val_loss: 1.4663 - val_sparse_categorical_accuracy: 0.6200\nEpoch 3/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.1610 - sparse_categorical_accuracy: 0.9375 - val_loss: 0.7033 - val_sparse_categorical_accuracy: 0.8000\nEpoch 4/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.1297 - sparse_categorical_accuracy: 0.9494 - val_loss: 0.4062 - val_sparse_categorical_accuracy: 0.8600\nEpoch 5/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.1061 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9550\nEpoch 6/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0898 - sparse_categorical_accuracy: 0.9681 - val_loss: 0.5251 - val_sparse_categorical_accuracy: 0.7600\nEpoch 7/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0848 - sparse_categorical_accuracy: 0.9688 - val_loss: 0.2458 - val_sparse_categorical_accuracy: 0.9050\nEpoch 8/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0727 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.1200 - val_sparse_categorical_accuracy: 0.9550\nEpoch 9/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0558 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.3578 - val_sparse_categorical_accuracy: 0.8700\nEpoch 10/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0506 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.1786 - val_sparse_categorical_accuracy: 0.9350\nEpoch 11/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0428 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.9050\nEpoch 12/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0379 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.1635 - val_sparse_categorical_accuracy: 0.9400\nEpoch 13/20\n80/80 [==============================] - 6s 81ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.1550 - val_sparse_categorical_accuracy: 0.9450\nEpoch 14/20\n80/80 [==============================] - 7s 91ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.1406 - val_sparse_categorical_accuracy: 0.9700\nEpoch 15/20\n80/80 [==============================] - 7s 90ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.1297 - val_sparse_categorical_accuracy: 0.9650\nEpoch 16/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1290 - val_sparse_categorical_accuracy: 0.9650\nEpoch 17/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1296 - val_sparse_categorical_accuracy: 0.9600\nEpoch 18/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1313 - val_sparse_categorical_accuracy: 0.9600\nEpoch 19/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1340 - val_sparse_categorical_accuracy: 0.9650\nEpoch 20/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1366 - val_sparse_categorical_accuracy: 0.9650\nTraining Loss: 0.0138\nTraining Accuracy: 99.69%\n\nValidation Loss: 0.1366\nValidation Accuracy: 96.50%\n\n10/10 [==============================] - 1s 56ms/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9850\nTest Loss: 0.0318\nTest Accuracy: 98.50%\nWARNING:absl:Found untraced functions such as conv2d_534_layer_call_fn, conv2d_534_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_534_layer_call_fn, re_lu_534_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055143/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055143/assets\n2022-12-14 21:59:09.147408: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 21:59:09.147450: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 21:59:09.147584: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671055143\n2022-12-14 21:59:09.156401: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 21:59:09.156431: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671055143\n2022-12-14 21:59:09.182280: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 21:59:09.274563: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671055143\n2022-12-14 21:59:09.306922: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 159338 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_179:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 98.500%\nModel size: 420.4KB\nZipped tflite size: 385.581 KB\nPreprocessing Latency: 5.0ms\nModel Latency: 1.2ms\nTotal Latency: 6.1ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.3, 'final_sparsity': 0.6}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:59:12.249643: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:12.251100: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:12.251301: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:59:12.855614: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:12.857105: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:12.857271: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:59:13.185865: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:13.187406: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:13.187600: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 21:59:13.524267: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:13.525810: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 21:59:13.525989: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0], shape=(20,), dtype=int64)\n2022-12-14 21:59:13.959531: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 12s 122ms/step - loss: 0.5325 - sparse_categorical_accuracy: 0.7356 - val_loss: 1.5999 - val_sparse_categorical_accuracy: 0.6800\nEpoch 2/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.2939 - sparse_categorical_accuracy: 0.8869 - val_loss: 0.2730 - val_sparse_categorical_accuracy: 0.8950\nEpoch 3/20\n80/80 [==============================] - 6s 76ms/step - loss: 0.1866 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.1746 - val_sparse_categorical_accuracy: 0.9350\nEpoch 4/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.1646 - sparse_categorical_accuracy: 0.9381 - val_loss: 0.2219 - val_sparse_categorical_accuracy: 0.9150\nEpoch 5/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.1209 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.1499 - val_sparse_categorical_accuracy: 0.9550\nEpoch 6/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.1163 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.1951 - val_sparse_categorical_accuracy: 0.9450\nEpoch 7/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0863 - sparse_categorical_accuracy: 0.9669 - val_loss: 0.1304 - val_sparse_categorical_accuracy: 0.9450\nEpoch 8/20\n80/80 [==============================] - 6s 76ms/step - loss: 0.0658 - sparse_categorical_accuracy: 0.9731 - val_loss: 0.1389 - val_sparse_categorical_accuracy: 0.9500\nEpoch 9/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0517 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.1543 - val_sparse_categorical_accuracy: 0.9550\nEpoch 10/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0435 - sparse_categorical_accuracy: 0.9862 - val_loss: 0.1538 - val_sparse_categorical_accuracy: 0.9400\nEpoch 11/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.1657 - val_sparse_categorical_accuracy: 0.9350\nEpoch 12/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0320 - sparse_categorical_accuracy: 0.9900 - val_loss: 0.2424 - val_sparse_categorical_accuracy: 0.9150\nEpoch 13/20\n80/80 [==============================] - 6s 76ms/step - loss: 0.0265 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.1860 - val_sparse_categorical_accuracy: 0.9450\nEpoch 14/20\n80/80 [==============================] - 6s 76ms/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.1650 - val_sparse_categorical_accuracy: 0.9300\nEpoch 15/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9937 - val_loss: 0.1509 - val_sparse_categorical_accuracy: 0.9550\nEpoch 16/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1354 - val_sparse_categorical_accuracy: 0.9600\nEpoch 17/20\n80/80 [==============================] - 6s 81ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1464 - val_sparse_categorical_accuracy: 0.9550\nEpoch 18/20\n80/80 [==============================] - 7s 83ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1406 - val_sparse_categorical_accuracy: 0.9600\nEpoch 19/20\n80/80 [==============================] - 7s 82ms/step - loss: 0.0140 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1376 - val_sparse_categorical_accuracy: 0.9600\nEpoch 20/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0130 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1424 - val_sparse_categorical_accuracy: 0.9600\nTraining Loss: 0.0130\nTraining Accuracy: 99.69%\n\nValidation Loss: 0.1424\nValidation Accuracy: 96.00%\n\n10/10 [==============================] - 1s 57ms/step - loss: 0.0385 - sparse_categorical_accuracy: 0.9800\nTest Loss: 0.0385\nTest Accuracy: 98.00%\nWARNING:absl:Found untraced functions such as conv2d_546_layer_call_fn, conv2d_546_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_546_layer_call_fn, re_lu_546_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055285/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055285/assets\n2022-12-14 22:01:31.920861: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 22:01:31.920897: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 22:01:31.921030: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671055285\n2022-12-14 22:01:31.933497: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 22:01:31.933533: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671055285\n2022-12-14 22:01:31.959486: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 22:01:32.076750: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671055285\n2022-12-14 22:01:32.111492: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 190463 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_183:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 98.000%\nModel size: 420.4KB\nZipped tflite size: 385.614 KB\nPreprocessing Latency: 5.4ms\nModel Latency: 1.3ms\nTotal Latency: 6.4ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.3, 'final_sparsity': 0.7}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:01:35.532023: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:35.533517: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:35.533702: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:01:36.158785: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:36.160282: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:36.160461: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:01:36.504924: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:36.506451: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:36.506641: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:01:36.863646: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:36.865166: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:01:36.865328: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0], shape=(20,), dtype=int64)\n2022-12-14 22:01:37.326496: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 13s 124ms/step - loss: 0.4872 - sparse_categorical_accuracy: 0.7669 - val_loss: 0.3926 - val_sparse_categorical_accuracy: 0.8800\nEpoch 2/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.9181 - val_loss: 1.4370 - val_sparse_categorical_accuracy: 0.6250\nEpoch 3/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.1640 - sparse_categorical_accuracy: 0.9375 - val_loss: 0.1740 - val_sparse_categorical_accuracy: 0.9350\nEpoch 4/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.1315 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.1027 - val_sparse_categorical_accuracy: 0.9600\nEpoch 5/20\n80/80 [==============================] - 6s 81ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9581 - val_loss: 0.1119 - val_sparse_categorical_accuracy: 0.9650\nEpoch 6/20\n80/80 [==============================] - 7s 82ms/step - loss: 0.1008 - sparse_categorical_accuracy: 0.9588 - val_loss: 0.1232 - val_sparse_categorical_accuracy: 0.9500\nEpoch 7/20\n80/80 [==============================] - 7s 81ms/step - loss: 0.0884 - sparse_categorical_accuracy: 0.9638 - val_loss: 0.2000 - val_sparse_categorical_accuracy: 0.9200\nEpoch 8/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0644 - sparse_categorical_accuracy: 0.9775 - val_loss: 0.1303 - val_sparse_categorical_accuracy: 0.9450\nEpoch 9/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9787 - val_loss: 0.1158 - val_sparse_categorical_accuracy: 0.9700\nEpoch 10/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.1246 - val_sparse_categorical_accuracy: 0.9600\nEpoch 11/20\n80/80 [==============================] - 6s 77ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.1257 - val_sparse_categorical_accuracy: 0.9700\nEpoch 12/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0304 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1290 - val_sparse_categorical_accuracy: 0.9700\nEpoch 13/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.1293 - val_sparse_categorical_accuracy: 0.9650\nEpoch 14/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.1272 - val_sparse_categorical_accuracy: 0.9650\nEpoch 15/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0173 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1252 - val_sparse_categorical_accuracy: 0.9650\nEpoch 16/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1349 - val_sparse_categorical_accuracy: 0.9650\nEpoch 17/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9956 - val_loss: 0.1380 - val_sparse_categorical_accuracy: 0.9650\nEpoch 18/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1400 - val_sparse_categorical_accuracy: 0.9650\nEpoch 19/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1390 - val_sparse_categorical_accuracy: 0.9650\nEpoch 20/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1371 - val_sparse_categorical_accuracy: 0.9600\nTraining Loss: 0.0113\nTraining Accuracy: 99.62%\n\nValidation Loss: 0.1371\nValidation Accuracy: 96.00%\n\n10/10 [==============================] - 1s 56ms/step - loss: 0.0398 - sparse_categorical_accuracy: 0.9850\nTest Loss: 0.0398\nTest Accuracy: 98.50%\nWARNING:absl:Found untraced functions such as conv2d_558_layer_call_fn, conv2d_558_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_558_layer_call_fn, re_lu_558_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055435/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055435/assets\n2022-12-14 22:04:01.043691: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 22:04:01.043730: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 22:04:01.043864: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671055435\n2022-12-14 22:04:01.053462: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 22:04:01.053495: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671055435\n2022-12-14 22:04:01.082457: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 22:04:01.171768: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671055435\n2022-12-14 22:04:01.205655: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 161792 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_187:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 98.500%\nModel size: 420.4KB\nZipped tflite size: 385.540 KB\nPreprocessing Latency: 5.1ms\nModel Latency: 1.2ms\nTotal Latency: 6.1ms\nparameters are:  {'downsampling_rate': 16000, 'frame_length_in_s': 0.016, 'num_mel_bins': 20, 'lower_frequency': 20, 'upper_frequency': 8000, 'batch_size': 20, 'initial_learning_rate': 0.01, 'end_learning_rate': 1e-05, 'epochs': 20, 'model_filter': 256, 'alpha': 0.3, 'final_sparsity': 0.8}\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:04:04.162871: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:04.164335: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:04.164516: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nSHAPE =  (20, 20)\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:04:04.772043: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:04.773529: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:04.773710: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:04:05.109187: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:05.110732: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:05.110915: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-14 22:04:05.445089: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:05.446502: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-14 22:04:05.446688: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0], shape=(20,), dtype=int64)\n2022-12-14 22:04:05.925424: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nEpoch 1/20\n80/80 [==============================] - 12s 127ms/step - loss: 0.3834 - sparse_categorical_accuracy: 0.8338 - val_loss: 3.4874 - val_sparse_categorical_accuracy: 0.5200\nEpoch 2/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.1881 - sparse_categorical_accuracy: 0.9300 - val_loss: 5.5402 - val_sparse_categorical_accuracy: 0.5000\nEpoch 3/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.1379 - sparse_categorical_accuracy: 0.9494 - val_loss: 0.7484 - val_sparse_categorical_accuracy: 0.7850\nEpoch 4/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.1009 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.1572 - val_sparse_categorical_accuracy: 0.9400\nEpoch 5/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0892 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.3107 - val_sparse_categorical_accuracy: 0.8900\nEpoch 6/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0614 - sparse_categorical_accuracy: 0.9769 - val_loss: 0.2936 - val_sparse_categorical_accuracy: 0.8900\nEpoch 7/20\n80/80 [==============================] - 6s 78ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.3083 - val_sparse_categorical_accuracy: 0.9050\nEpoch 8/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0443 - sparse_categorical_accuracy: 0.9844 - val_loss: 0.3950 - val_sparse_categorical_accuracy: 0.9000\nEpoch 9/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0607 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.1737 - val_sparse_categorical_accuracy: 0.9550\nEpoch 10/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.2287 - val_sparse_categorical_accuracy: 0.9450\nEpoch 11/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9831 - val_loss: 0.2999 - val_sparse_categorical_accuracy: 0.9250\nEpoch 12/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0298 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.3208 - val_sparse_categorical_accuracy: 0.9300\nEpoch 13/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0256 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.2329 - val_sparse_categorical_accuracy: 0.9550\nEpoch 14/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.2002 - val_sparse_categorical_accuracy: 0.9550\nEpoch 15/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1780 - val_sparse_categorical_accuracy: 0.9600\nEpoch 16/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.1838 - val_sparse_categorical_accuracy: 0.9600\nEpoch 17/20\n80/80 [==============================] - 6s 79ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9950 - val_loss: 0.1900 - val_sparse_categorical_accuracy: 0.9600\nEpoch 18/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9962 - val_loss: 0.1889 - val_sparse_categorical_accuracy: 0.9600\nEpoch 19/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0113 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.1906 - val_sparse_categorical_accuracy: 0.9650\nEpoch 20/20\n80/80 [==============================] - 6s 80ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.1947 - val_sparse_categorical_accuracy: 0.9650\nTraining Loss: 0.0101\nTraining Accuracy: 99.87%\n\nValidation Loss: 0.1947\nValidation Accuracy: 96.50%\n\n10/10 [==============================] - 1s 55ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9950\nTest Loss: 0.0178\nTest Accuracy: 99.50%\nWARNING:absl:Found untraced functions such as conv2d_570_layer_call_fn, conv2d_570_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_570_layer_call_fn, re_lu_570_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055579/assets\nINFO:tensorflow:Assets written to: ./team10_final_8000/saved_models_grid_search/1671055579/assets\n2022-12-14 22:06:26.594165: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-14 22:06:26.594204: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-14 22:06:26.594349: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./team10_final_8000/saved_models_grid_search/1671055579\n2022-12-14 22:06:26.603103: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-14 22:06:26.603133: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./team10_final_8000/saved_models_grid_search/1671055579\n2022-12-14 22:06:26.629761: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-14 22:06:26.723619: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./team10_final_8000/saved_models_grid_search/1671055579\n2022-12-14 22:06:26.758028: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 163680 microseconds.\nNumber of inputs: 1\nNumber of outputs: 1\nInput name: serving_default_input_191:0\nInput shape: [ 1 32 32  1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 2]\nfilename len: 200\nAccuracy: 99.500%\nModel size: 420.4KB\nZipped tflite size: 385.787 KB\nPreprocessing Latency: 5.6ms\nModel Latency: 1.3ms\nTotal Latency: 6.7ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5c74a7a1-d505-4b2b-a4fa-efea0c0f20da' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"b92542970f8842f291e03a550632cc63","deepnote_execution_queue":[]}}