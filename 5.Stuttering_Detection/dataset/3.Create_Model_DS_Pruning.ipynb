{"cells":[{"cell_type":"code","metadata":{"tags":[],"source_hash":"55bf1a00","execution_start":1670423140954,"execution_millis":6307,"deepnote_to_be_reexecuted":false,"cell_id":"91b414028d3f43b59f567b4a821002ec","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport tensorflow_model_optimization as tfmot\nfrom preprocessing import *\nfrom functools import partial\nfrom typing import Iterable, Any\nfrom itertools import product\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport random\nimport zipfile\nfrom glob import glob\nimport os\nfrom sklearn.model_selection import ParameterGrid","execution_count":null,"outputs":[{"name":"stderr","text":"2022-12-07 14:25:40.964472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-07 14:25:41.070870: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-12-07 14:25:41.075727: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-12-07 14:25:41.075745: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-07 14:25:41.101142: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-12-07 14:25:42.853642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-12-07 14:25:42.853702: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-12-07 14:25:42.853707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"aae97180","execution_start":1670423147310,"execution_millis":0,"deepnote_to_be_reexecuted":false,"cell_id":"886655545a1d4062a3940c6740827e6a","deepnote_cell_type":"code"},"source":"PREPROCESSING_ARGS = {\n    'downsampling_rate': 16000,\n    'frame_length_in_s': 0.04,\n    'frame_step_in_s': 0.02,\n    'num_mel_bins': 40,\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n    'num_coefficients':40\n}\n\nTRAINING_ARGS = {\n    'batch_size': 20,\n    'epochs': 10,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 1.e-5,\n}\n\nfinal_sparsity = 0.70","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"c200b16b","execution_start":1670423147310,"execution_millis":1367,"deepnote_to_be_reexecuted":false,"cell_id":"cc531eae51b441f9b8aa0ed29831a8d1","deepnote_cell_type":"code"},"source":"train_ds = tf.data.Dataset.list_files(['msc-train/go*', 'msc-train/stop*'])\nval_ds = tf.data.Dataset.list_files(['msc-val/go*', 'msc-val/stop*'])\ntest_ds = tf.data.Dataset.list_files(['msc-test/go*', 'msc-test/stop*'])\ngo_stop_label = LABELS\n\ndef get_mfcc_and_label(filename, downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency, num_coefficients):\n    mfccs, label = get_mfccs(filename, downsampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency, num_coefficients)\n    \n    return mfccs, label\n\nget_frozen_spectrogram = partial(get_mfcc_and_label, **PREPROCESSING_ARGS)\n\nfor spectrogram, label in train_ds.map(get_frozen_spectrogram).take(1):\n    SHAPE = spectrogram.shape\nSHAPE\n\n","execution_count":null,"outputs":[{"name":"stderr","text":"2022-12-07 14:25:47.269431: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-12-07 14:25:47.269466: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-07 14:25:47.269483: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-d234ae94-1d7d-4b6d-92da-bcf2d3a86b11): /proc/driver/nvidia/version does not exist\n2022-12-07 14:25:47.269752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-07 14:25:47.768363: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2022-12-07 14:25:47.768581: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-07 14:25:48.103045: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:48.104620: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:48.104796: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"TensorShape([49, 40])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"6447314a","execution_start":1670423148680,"execution_millis":1739,"deepnote_to_be_reexecuted":false,"cell_id":"6f023c9b7809440c86ac2f22405552ab","deepnote_cell_type":"code"},"source":"def preprocess(filename):\n    signal, label = get_frozen_spectrogram(filename)\n\n    signal.set_shape(SHAPE)\n    signal = tf.expand_dims(signal, -1)\n    signal = tf.image.resize(signal, [32, 32])\n\n    label_id = tf.argmax(label == LABELS)\n\n    return signal, label_id\n\nbatch_size = TRAINING_ARGS['batch_size']\nepoch = TRAINING_ARGS['epochs']\n\ntrain_ds = train_ds.map(preprocess).batch(batch_size).cache()\nval_ds = val_ds.map(preprocess).batch(batch_size)\ntest_ds = test_ds.map(preprocess).batch(batch_size)\n\nfor example_batch, example_labels in train_ds.take(1):\n  print('Batch Shape:', example_batch.shape)\n  print('Data Shape:', example_batch.shape[1:])\n  print('Labels:', example_labels)","execution_count":null,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-07 14:25:48.910006: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:48.911845: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:48.912033: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-07 14:25:49.278474: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:49.280152: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:49.280333: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-07 14:25:49.631760: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:49.633277: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-07 14:25:49.633466: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nBatch Shape: (20, 32, 32, 1)\nData Shape: (32, 32, 1)\nLabels: tf.Tensor([0 1 0 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1], shape=(20,), dtype=int64)\n2022-12-07 14:25:50.347865: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"88bb1303","execution_start":1670423150445,"execution_millis":729,"deepnote_to_be_reexecuted":false,"cell_id":"c8abe7a87bb44bd990b20b374dfbacb5","deepnote_cell_type":"code"},"source":"model_DS = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[2, 2],\n        use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=128, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=128, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=len(LABELS)),\n    tf.keras.layers.Softmax()\n    ])\n  \nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\nbegin_step = int(len(train_ds) * epoch * 0.2)\nend_step = int(len(train_ds) * epoch)\n\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.20,\n        final_sparsity=final_sparsity,\n        begin_step=begin_step,\n        end_step=end_step\n    )\n    }\n\nmodel_for_pruning = prune_low_magnitude(model_DS, **pruning_params)\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"59efc34f","execution_start":1670423151178,"execution_millis":70,"deepnote_to_be_reexecuted":false,"cell_id":"18ebe19a2626488c839ee5b68ff5766b","deepnote_cell_type":"code"},"source":"model_for_pruning.summary()","execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n prune_low_magnitude_conv2d   (None, 15, 15, 128)      2306      \n (PruneLowMagnitude)                                             \n                                                                 \n prune_low_magnitude_batch_n  (None, 15, 15, 128)      513       \n ormalization (PruneLowMagni                                     \n tude)                                                           \n                                                                 \n prune_low_magnitude_re_lu (  (None, 15, 15, 128)      1         \n PruneLowMagnitude)                                              \n                                                                 \n prune_low_magnitude_depthwi  (None, 15, 15, 128)      1153      \n se_conv2d (PruneLowMagnitud                                     \n e)                                                              \n                                                                 \n prune_low_magnitude_conv2d_  (None, 15, 15, 128)      32770     \n 1 (PruneLowMagnitude)                                           \n                                                                 \n prune_low_magnitude_batch_n  (None, 15, 15, 128)      513       \n ormalization_1 (PruneLowMag                                     \n nitude)                                                         \n                                                                 \n prune_low_magnitude_re_lu_1  (None, 15, 15, 128)      1         \n  (PruneLowMagnitude)                                            \n                                                                 \n prune_low_magnitude_depthwi  (None, 15, 15, 128)      1153      \n se_conv2d_1 (PruneLowMagnit                                     \n ude)                                                            \n                                                                 \n prune_low_magnitude_conv2d_  (None, 15, 15, 128)      32770     \n 2 (PruneLowMagnitude)                                           \n                                                                 \n prune_low_magnitude_batch_n  (None, 15, 15, 128)      513       \n ormalization_2 (PruneLowMag                                     \n nitude)                                                         \n                                                                 \n prune_low_magnitude_re_lu_2  (None, 15, 15, 128)      1         \n  (PruneLowMagnitude)                                            \n                                                                 \n prune_low_magnitude_global_  (None, 128)              1         \n average_pooling2d (PruneLow                                     \n Magnitude)                                                      \n                                                                 \n prune_low_magnitude_dense (  (None, 2)                516       \n PruneLowMagnitude)                                              \n                                                                 \n prune_low_magnitude_softmax  (None, 2)                1         \n  (PruneLowMagnitude)                                            \n                                                                 \n=================================================================\nTotal params: 72,212\nTrainable params: 37,250\nNon-trainable params: 34,962\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"48f91d14","execution_start":1670423151303,"execution_millis":81364,"deepnote_to_be_reexecuted":false,"cell_id":"c3fcf63009d94a4495a2742683fc2542","deepnote_cell_type":"code"},"source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epoch,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\nmodel_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model_for_pruning.fit(train_ds, epochs=epoch, validation_data=val_ds, callbacks=callbacks)\nfor layer in model_for_pruning.layers:\n    if isinstance(layer, tf.keras.layers.Wrapper):\n        weights = layer.trainable_weights\n    else:\n        weights = layer.weights\n\ntest_loss, test_accuracy =  model_for_pruning.evaluate(test_ds)\n\ntraining_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\nval_loss = history.history['val_loss'][-1]\nval_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n\nprint(f'Training loss: {training_loss:.3f}')\nprint(f'Training accuracy: {training_accuracy * 100:.2f}%')\nprint(f'Validation loss: {val_loss:.3f}')\nprint(f'Validation accuracy: {val_accuracy * 100:.2f}%')\nprint(f'Test loss: {test_loss:.3f}')\nprint(f'Test accuracy: {test_accuracy * 100:.2f}%')","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n80/80 [==============================] - 26s 280ms/step - loss: 0.5236 - sparse_categorical_accuracy: 0.7487 - val_loss: 0.3835 - val_sparse_categorical_accuracy: 0.8300\nEpoch 2/10\n80/80 [==============================] - 6s 75ms/step - loss: 0.3807 - sparse_categorical_accuracy: 0.8394 - val_loss: 0.3537 - val_sparse_categorical_accuracy: 0.8950\nEpoch 3/10\n80/80 [==============================] - 6s 73ms/step - loss: 0.2860 - sparse_categorical_accuracy: 0.8869 - val_loss: 0.6954 - val_sparse_categorical_accuracy: 0.7100\nEpoch 4/10\n80/80 [==============================] - 6s 72ms/step - loss: 0.2284 - sparse_categorical_accuracy: 0.9087 - val_loss: 0.2591 - val_sparse_categorical_accuracy: 0.9200\nEpoch 5/10\n80/80 [==============================] - 6s 74ms/step - loss: 0.1743 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.4725 - val_sparse_categorical_accuracy: 0.8050\nEpoch 6/10\n80/80 [==============================] - 6s 72ms/step - loss: 0.1538 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.2638 - val_sparse_categorical_accuracy: 0.8950\nEpoch 7/10\n80/80 [==============================] - 6s 73ms/step - loss: 0.1375 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.2099 - val_sparse_categorical_accuracy: 0.9350\nEpoch 8/10\n80/80 [==============================] - 6s 73ms/step - loss: 0.1279 - sparse_categorical_accuracy: 0.9544 - val_loss: 0.1483 - val_sparse_categorical_accuracy: 0.9500\nEpoch 9/10\n80/80 [==============================] - 6s 74ms/step - loss: 0.1163 - sparse_categorical_accuracy: 0.9575 - val_loss: 0.1784 - val_sparse_categorical_accuracy: 0.9400\nEpoch 10/10\n80/80 [==============================] - 6s 73ms/step - loss: 0.0968 - sparse_categorical_accuracy: 0.9688 - val_loss: 0.1615 - val_sparse_categorical_accuracy: 0.9500\n10/10 [==============================] - 3s 212ms/step - loss: 0.1756 - sparse_categorical_accuracy: 0.9250\nTraining loss: 0.097\nTraining accuracy: 96.88%\nValidation loss: 0.161\nValidation accuracy: 95.00%\nTest loss: 0.176\nTest accuracy: 92.50%\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"985149b6","execution_start":1670423424009,"execution_millis":4595,"deepnote_to_be_reexecuted":false,"cell_id":"2023b8d6b6e6433395d1d0c330121027","deepnote_cell_type":"code"},"source":"from time import time\n#SAVE THE MODEL\ntimestamp = int(time())\n\nsaved_model_dir = f'./saved_models/{timestamp}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel_for_pruning.save(saved_model_dir)","execution_count":null,"outputs":[{"name":"stderr","text":"WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./saved_models/1670423423/assets\nINFO:tensorflow:Assets written to: ./saved_models/1670423423/assets\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"575046d1","execution_start":1670423632079,"execution_millis":2320,"deepnote_to_be_reexecuted":false,"cell_id":"77e90e89b0fb418ab9825ba1e8c978a0","deepnote_cell_type":"code"},"source":"MODEL_NAME=saved_model_dir.split('/')[2]\n#CONVERT THE MODEL TO TFLITE\nconverter = tf.lite.TFLiteConverter.from_saved_model(f'{saved_model_dir}')\ntflite_model = converter.convert()\ntflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","execution_count":null,"outputs":[{"name":"stderr","text":"2022-12-07 14:33:53.886423: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-07 14:33:53.886467: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-07 14:33:53.886602: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/1670423423\n2022-12-07 14:33:53.897277: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-07 14:33:53.897317: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/1670423423\n2022-12-07 14:33:53.936045: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-07 14:33:54.044942: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/1670423423\n2022-12-07 14:33:54.083076: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 196473 microseconds.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"9658f457","execution_start":1670423675976,"execution_millis":14,"deepnote_to_be_reexecuted":false,"cell_id":"bf16f0f6292345babc2673d5aab97f7e","deepnote_cell_type":"code"},"source":"tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\ntflite_model_name","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"'./tflite_models/1670423423.tflite'"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"source_hash":"404a9ed9","execution_start":1670423237264,"execution_millis":3336,"deepnote_to_be_reexecuted":false,"cell_id":"7cdf0a509ca044e3821c22b2e95b2f07","deepnote_cell_type":"code"},"source":"with open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model)\n    \nimport zipfile\n#COMPRESS THE MODEL\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name)\n\ntflite_size = os.path.getsize(tflite_model_name) / 1024.0  \nzipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0\n\nprint(f'Original tflite size (pruned model): {tflite_size:.3f} KB')\nprint(f'Zipped tflite size (pruned model): {zipped_size:.3f} KB')\n","execution_count":null,"outputs":[{"name":"stderr","text":"2022-12-07 14:27:19.132871: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-07 14:27:19.132923: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-07 14:27:19.133753: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/1670423232\n2022-12-07 14:27:19.144544: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-07 14:27:19.144580: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/1670423232\n2022-12-07 14:27:19.190154: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n2022-12-07 14:27:19.197916: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-07 14:27:19.311861: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/1670423232\n2022-12-07 14:27:19.352400: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 218652 microseconds.\n2022-12-07 14:27:19.462475: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './tflite_models/1670423232.tflite'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#COMPRESS THE MODEL\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtflite_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, compression\u001b[38;5;241m=\u001b[39mzipfile\u001b[38;5;241m.\u001b[39mZIP_DEFLATED) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtflite_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m tflite_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(tflite_model_name) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024.0\u001b[39m  \n\u001b[1;32m     18\u001b[0m zipped_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtflite_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.9/zipfile.py:1736\u001b[0m, in \u001b[0;36mZipFile.write\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writing:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt write to ZIP archive while an open writing handle exists\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1734\u001b[0m     )\n\u001b[0;32m-> 1736\u001b[0m zinfo \u001b[38;5;241m=\u001b[39m \u001b[43mZipInfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstrict_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strict_timestamps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m zinfo\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m   1740\u001b[0m     zinfo\u001b[38;5;241m.\u001b[39mcompress_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.9/zipfile.py:501\u001b[0m, in \u001b[0;36mZipInfo.from_file\u001b[0;34m(cls, filename, arcname, strict_timestamps)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[1;32m    500\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(filename)\n\u001b[0;32m--> 501\u001b[0m st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m isdir \u001b[38;5;241m=\u001b[39m stat\u001b[38;5;241m.\u001b[39mS_ISDIR(st\u001b[38;5;241m.\u001b[39mst_mode)\n\u001b[1;32m    503\u001b[0m mtime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mlocaltime(st\u001b[38;5;241m.\u001b[39mst_mtime)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tflite_models/1670423232.tflite'"]}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5c74a7a1-d505-4b2b-a4fa-efea0c0f20da' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"0f28566679c843658556fb257a893c98","deepnote_execution_queue":[]}}